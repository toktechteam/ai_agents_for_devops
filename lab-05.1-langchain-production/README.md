# Lab 5.1 FREE Version â€“ Production-Style LangChain Agent Patterns
## Learning LangChain Operational Patterns Without the Complexity

---

## ğŸ¯ What You Will Learn

### Core Concepts

By completing this lab, you will master:

1. **LangChain Production Patterns** - Without needing LangChain installed:
   - **Chain-style execution**: Multi-step agent workflows
   - **Tool orchestration**: Sequential and parallel execution
   - **State management**: Conversation and decision history
   - **Cost tracking**: Token usage and financial monitoring
   - **Metrics exposure**: Prometheus-ready observability

2. **Production Agent Deployment** - Real-world operational patterns:
   - **Horizontal scaling**: Multi-replica deployments
   - **Health monitoring**: Readiness and liveness probes
   - **Resource management**: CPU/memory limits for agents
   - **Autoscaling**: HPA based on load
   - **Zero-downtime**: Rolling updates

3. **Infrastructure Economics** - Cost-aware ML systems:
   - **Token tracking**: Per-request token consumption
   - **Cost calculation**: Real-time cost per investigation
   - **Budget management**: Cost limits and alerting
   - **Optimization strategies**: Reducing operational costs

4. **Agent Chain Architecture** - Multi-step reasoning:
   - **Planner**: Analyzes problem and creates plan
   - **Executor**: Runs tools in sequence
   - **Reasoner**: Synthesizes results
   - **Memory**: Maintains context across calls

### Practical Skills

You will be able to:

- âœ… Implement chain-style agent execution flows
- âœ… Track token usage and costs in real-time
- âœ… Build Prometheus metrics for ML systems
- âœ… Deploy horizontally scalable agents
- âœ… Implement decision logging for auditing
- âœ… Design tool execution pipelines
- âœ… Manage agent state in memory
- âœ… Configure autoscaling for agent workloads

### Real-World Applications

**ML Platform Engineers** will learn:
- Production deployment patterns for LangChain agents
- Cost tracking and optimization strategies
- Horizontal scaling for agent workloads
- Observability for AI systems

**SREs** will learn:
- Running AI agents in production
- Monitoring token usage and costs
- Autoscaling agent deployments
- Reliability patterns for ML services

**FinOps Teams** will learn:
- Tracking ML inference costs
- Token-level cost attribution
- Budget management for AI services
- Cost optimization opportunities

---

## ğŸ“‹ Prerequisites

### Required Software
- **Operating System:** Ubuntu 22.04 (or similar Linux / WSL2 / macOS)
- **Docker:** Version 24 or higher
- **kind:** Kubernetes in Docker
- **kubectl:** Version 1.29 or higher
- **Python:** Version 3.11 or higher
- **curl:** For API testing

### Required Knowledge
- Basic understanding of LangChain concepts (chains, tools, agents)
- Kubernetes fundamentals (deployments, services, HPA)
- Prometheus metrics basics
- Cost management principles

### Important Note

**This lab does NOT require:**
- âŒ LangChain installation
- âŒ OpenAI API key
- âŒ GPU resources
- âŒ Redis or Postgres
- âŒ Vector databases

**Why?** This free version focuses on **operational patterns** you need to run LangChain agents in production, not the LangChain library itself. You'll learn the infrastructure side.

---

## ğŸ—ï¸ Architecture Overview

### What You're Building

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Namespace: langchain-free                     â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚  Deployment: langchain-free (3 replicas)     â”‚    â”‚  â”‚
â”‚  â”‚  â”‚                                              â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  Pod 1: Chain Agent                    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  FastAPI (Port 8000)             â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ /investigate                 â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ /metrics (Prometheus)        â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ /health                      â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚                 â”‚                       â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚                 â–¼                       â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Chain Engine                    â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚                                  â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Step 1: Planner                 â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Analyze alert                â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Generate plan                â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Track tokens used            â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚                                  â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Step 2: Executor                â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Run tools in sequence        â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Collect results              â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Track tokens used            â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚                                  â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Step 3: Reasoner                â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Synthesize findings          â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â”œâ”€ Generate summary             â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  â””â”€ Track tokens used            â”‚  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚                 â”‚                       â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚      â”‚                    â”‚            â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚      â–¼                    â–¼            â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ Tools   â”‚      â”‚ Cost Tracker â”‚    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚         â”‚      â”‚              â”‚    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ - check â”‚      â”‚ - Tokens usedâ”‚    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚   _pods â”‚      â”‚ - Cost (USD) â”‚    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ - get   â”‚      â”‚ - Per chain  â”‚    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚   _logs â”‚      â”‚ - Cumulative â”‚    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚ - get   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚   _metric                           â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚                                        â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Memory Store (In-process)       â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - Last alert                    â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - Investigation history         â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - Service patterns              â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚                                        â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Metrics Exporter                â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - agent_requests_total          â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - agent_tokens_total            â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - agent_cost_total              â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  - agent_latency_seconds         â”‚ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚  â”‚
â”‚  â”‚  â”‚                                              â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  Pod 2 and Pod 3: Same structure            â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚  Service: langchain-free (LoadBalancer)      â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  Distributes traffic across 3 pods           â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚  HPA: Horizontal Pod Autoscaler              â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  Min: 1, Max: 5, Target CPU: 70%             â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Chain Execution Flow

```
Investigation Request
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: PLANNER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input: Alert                      â”‚  â”‚
â”‚  â”‚ {                                 â”‚  â”‚
â”‚  â”‚   "alert_type": "high_latency",   â”‚  â”‚
â”‚  â”‚   "service": "checkout-api"       â”‚  â”‚
â”‚  â”‚ }                                 â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Process:                          â”‚  â”‚
â”‚  â”‚ - Analyze alert type              â”‚  â”‚
â”‚  â”‚ - Determine required tools        â”‚  â”‚
â”‚  â”‚ - Generate plan                   â”‚  â”‚
â”‚  â”‚ - Simulate LLM token usage        â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Output:                           â”‚  â”‚
â”‚  â”‚ {                                 â”‚  â”‚
â”‚  â”‚   "plan": [                       â”‚  â”‚
â”‚  â”‚     "check_pods",                 â”‚  â”‚
â”‚  â”‚     "get_metrics",                â”‚  â”‚
â”‚  â”‚     "get_logs"                    â”‚  â”‚
â”‚  â”‚   ],                              â”‚  â”‚
â”‚  â”‚   "tokens_used": 45               â”‚  â”‚
â”‚  â”‚ }                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: EXECUTOR                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ For each tool in plan:            â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Tool 1: check_pods                â”‚  â”‚
â”‚  â”‚ Result: "3 pods running,          â”‚  â”‚
â”‚  â”‚          1 high latency"          â”‚  â”‚
â”‚  â”‚ Tokens: 12                        â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Tool 2: get_metrics               â”‚  â”‚
â”‚  â”‚ Result: "p95: 480ms (target:      â”‚  â”‚
â”‚  â”‚          200ms)"                  â”‚  â”‚
â”‚  â”‚ Tokens: 15                        â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Tool 3: get_logs                  â”‚  â”‚
â”‚  â”‚ Result: "ERROR: Database timeout" â”‚  â”‚
â”‚  â”‚ Tokens: 18                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: REASONER                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Input: All tool results           â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Process:                          â”‚  â”‚
â”‚  â”‚ - Analyze findings                â”‚  â”‚
â”‚  â”‚ - Identify root cause             â”‚  â”‚
â”‚  â”‚ - Generate recommendations        â”‚  â”‚
â”‚  â”‚ - Simulate LLM token usage        â”‚  â”‚
â”‚  â”‚                                   â”‚  â”‚
â”‚  â”‚ Output:                           â”‚  â”‚
â”‚  â”‚ {                                 â”‚  â”‚
â”‚  â”‚   "summary": "Database timeout    â”‚  â”‚
â”‚  â”‚    causing high latency",         â”‚  â”‚
â”‚  â”‚   "root_cause": "Connection pool  â”‚  â”‚
â”‚  â”‚    exhaustion",                   â”‚  â”‚
â”‚  â”‚   "recommendations": [            â”‚  â”‚
â”‚  â”‚     "Increase connection pool",   â”‚  â”‚
â”‚  â”‚     "Add timeout monitoring"      â”‚  â”‚
â”‚  â”‚   ],                              â”‚  â”‚
â”‚  â”‚   "tokens_used": 67               â”‚  â”‚
â”‚  â”‚ }                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Report + Cost Tracking
{
  "total_tokens": 157,
  "cost_usd": 0.000314,
  "execution_time_ms": 234
}
```

---

## ğŸ“ Repository Structure

```
lab-05.1-langchain-production-free/
â”œâ”€â”€ README.md                   â† This file
â”œâ”€â”€ setup.md                    â† Detailed setup guide
â”œâ”€â”€ kind-cluster.yaml           â† Cluster configuration
â”œâ”€â”€ Dockerfile                  â† Container image definition
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 â† FastAPI application
â”‚   â”œâ”€â”€ chain_engine.py         â† Chain execution (Plannerâ†’Executorâ†’Reasoner)
â”‚   â”œâ”€â”€ tools.py                â† Simulated infrastructure tools
â”‚   â”œâ”€â”€ cost_tracker.py         â† Token usage and cost calculation
â”‚   â”œâ”€â”€ memory.py               â† In-process state management
â”‚   â”œâ”€â”€ metrics.py              â† Prometheus metrics exporter
â”‚   â”œâ”€â”€ config.py               â† Configuration management
â”‚   â”œâ”€â”€ requirements.txt        â† Python dependencies
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_main.py        â† API tests
â”‚       â”œâ”€â”€ test_chain.py       â† Chain execution tests
â”‚       â”œâ”€â”€ test_tools.py       â† Tool tests
â”‚       â”œâ”€â”€ test_cost.py        â† Cost tracking tests
â”‚       â””â”€â”€ test_metrics.py     â† Metrics tests
â””â”€â”€ k8s/
    â”œâ”€â”€ namespace.yaml          â† Namespace isolation
    â”œâ”€â”€ deployment.yaml         â† 3-replica deployment
    â”œâ”€â”€ service.yaml            â† Load balancer service
    â””â”€â”€ hpa.yaml                â† Horizontal Pod Autoscaler
```

---

## ğŸš€ Quick Start Guide

### Option 1: Run Locally

**Step 1: Navigate to app directory**
```bash
cd lab-05.1-langchain-production-free/app
```

**Step 2: Install dependencies**
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**Step 3: Start the agent**
```bash
uvicorn main:app --reload
```

**Step 4: Test investigation**

```bash
curl -X POST http://localhost:8000/investigate \
  -H "Content-Type: application/json" \
  -d '{
    "alert_type": "high_latency",
    "service": "checkout-api"
  }'
```

**Expected Response:**
```json
{
  "investigation_id": "inv_abc123",
  "alert": {
    "alert_type": "high_latency",
    "service": "checkout-api",
    "timestamp": "2024-01-15T10:30:00Z"
  },
  "chain_execution": {
    "planner": {
      "plan": ["check_pods", "get_metrics", "get_logs"],
      "reasoning": "High latency requires checking pod health, metrics, and logs",
      "tokens_used": 45
    },
    "executor": {
      "steps": [
        {
          "step": 1,
          "tool": "check_pods",
          "result": "[SIMULATED] 3 pods running: checkout-api-1, checkout-api-2, checkout-api-3. Pod checkout-api-2 showing high response time.",
          "tokens_used": 12,
          "duration_ms": 45
        },
        {
          "step": 2,
          "tool": "get_metrics",
          "result": "[SIMULATED] Metrics for checkout-api: p50=120ms, p95=480ms, p99=890ms. Target: p95 < 200ms.",
          "tokens_used": 15,
          "duration_ms": 32
        },
        {
          "step": 3,
          "tool": "get_logs",
          "result": "[SIMULATED] Recent errors: ERROR: Database connection timeout after 5000ms. Connection pool: 45/50 in use.",
          "tokens_used": 18,
          "duration_ms": 38
        }
      ],
      "total_tokens": 45
    },
    "reasoner": {
      "summary": "The checkout-api service is experiencing high latency (p95: 480ms vs target 200ms) due to database connection timeouts. Connection pool is near exhaustion (45/50 connections in use).",
      "root_cause": "Database connection pool exhaustion leading to timeout errors",
      "recommendations": [
        "Increase database connection pool size from 50 to 100",
        "Add connection timeout monitoring and alerts",
        "Review slow queries causing long-held connections",
        "Consider adding connection pooling at application layer"
      ],
      "confidence": 0.85,
      "tokens_used": 67
    }
  },
  "cost_tracking": {
    "total_tokens": 157,
    "breakdown": {
      "planner": 45,
      "executor": 45,
      "reasoner": 67
    },
    "cost_usd": 0.000314,
    "model": "simulated-gpt-4"
  },
  "execution_time_ms": 234,
  "memory_updated": true
}
```

**Step 5: View metrics**
```bash
curl http://localhost:8000/metrics
```

**Expected Output:**
```
# HELP agent_requests_total Total investigation requests
# TYPE agent_requests_total counter
agent_requests_total 1

# HELP agent_tokens_total Total tokens consumed
# TYPE agent_tokens_total counter
agent_tokens_total 157

# HELP agent_cost_total Total cost in USD
# TYPE agent_cost_total counter
agent_cost_total 0.000314

# HELP agent_latency_seconds Investigation latency
# TYPE agent_latency_seconds histogram
agent_latency_seconds_bucket{le="0.1"} 0
agent_latency_seconds_bucket{le="0.5"} 1
agent_latency_seconds_bucket{le="1.0"} 1
agent_latency_seconds_sum 0.234
agent_latency_seconds_count 1
```

---

### Option 2: Run on Kubernetes

**Step 1: Create kind cluster**
```bash
cd lab-05.1-langchain-production-free
kind create cluster --config kind-cluster.yaml
```

**Step 2: Build and load image**
```bash
docker build -t langchain-free:v1 .
kind load docker-image langchain-free:v1 --name langchain-free
```

**Step 3: Deploy**
```bash
kubectl apply -f k8s/
```

**Step 4: Wait for pods**
```bash
kubectl wait --for=condition=available --timeout=60s deployment/langchain-free -n langchain-free
```

**Step 5: Port-forward and test**
```bash
kubectl -n langchain-free port-forward svc/langchain-free 8000:8000

# In another terminal
curl -X POST http://localhost:8000/investigate \
  -H "Content-Type: application/json" \
  -d '{"alert_type":"high_cpu","service":"payment-api"}'
```

---

## ğŸ“Š Understanding Production Patterns

### 1. Chain-Style Execution

**Why chains matter:**
- **Structured reasoning**: Breaking complex tasks into steps
- **Observability**: Each step is trackable
- **Error handling**: Failures are isolated
- **Optimization**: Steps can be cached or parallelized

**Chain pattern:**
```python
class ChainEngine:
    def execute(self, alert):
        # Step 1: Planning
        plan = self.planner.generate_plan(alert)
        
        # Step 2: Execution
        results = self.executor.run_tools(plan)
        
        # Step 3: Reasoning
        summary = self.reasoner.synthesize(results)
        
        return {
            "plan": plan,
            "results": results,
            "summary": summary
        }
```

### 2. Token Tracking and Cost Management

**Token simulation:**
```python
class CostTracker:
    def __init__(self):
        self.cost_per_1k_tokens = 0.002  # $0.002 per 1K tokens (GPT-4 pricing)
    
    def track_step(self, step_name, input_text, output_text):
        # Simulate token counting
        tokens = len(input_text.split()) + len(output_text.split())
        cost = (tokens / 1000) * self.cost_per_1k_tokens
        
        return {
            "step": step_name,
            "tokens": tokens,
            "cost_usd": cost
        }
```

**Why this matters in production:**
- LLM costs can be 70-90% of operational expenses
- Token tracking enables cost attribution
- Budget alerts prevent runaway costs
- Optimization opportunities become visible

### 3. Prometheus Metrics

**Exposed metrics:**
```
agent_requests_total          - Total investigations
agent_tokens_total            - Total tokens consumed
agent_cost_total             - Total cost (USD)
agent_latency_seconds        - Request latency histogram
agent_errors_total           - Error count
agent_tools_executed_total   - Tool execution count
```

**Production monitoring:**
```yaml
# Prometheus scrape config
scrape_configs:
  - job_name: 'langchain-agent'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['langchain-free']
    metrics_path: '/metrics'
```

### 4. Horizontal Scaling

**Why 3 replicas?**
- **High availability**: No single point of failure
- **Load distribution**: Traffic spread across pods
- **Rolling updates**: Zero downtime deployments
- **Resource efficiency**: Better resource utilization

**HPA configuration:**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langchain-free
spec:
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
```

---

## ğŸ§ª Running Tests

```bash
cd lab-05.1-langchain-production-free/app
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pytest -v
```

**Expected Output:**
```
================== test session starts ==================
collected 12 items

tests/test_main.py::test_health_endpoint PASSED        [8%]
tests/test_main.py::test_investigate_endpoint PASSED   [16%]
tests/test_chain.py::test_planner PASSED               [25%]
tests/test_chain.py::test_executor PASSED              [33%]
tests/test_chain.py::test_reasoner PASSED              [41%]
tests/test_tools.py::test_check_pods PASSED            [50%]
tests/test_tools.py::test_get_metrics PASSED           [58%]
tests/test_cost.py::test_token_tracking PASSED         [66%]
tests/test_cost.py::test_cost_calculation PASSED       [75%]
tests/test_metrics.py::test_metrics_export PASSED      [83%]
tests/test_metrics.py::test_prometheus_format PASSED   [91%]
tests/test_metrics.py::test_metric_values PASSED       [100%]

=================== 12 passed in 1.85s ===================
```

---

## ğŸ’° Cost Analysis

### Running in KIND: $0/month

Completely free for learning and development.

### Production Deployment: $20-30/month

**Scenario:** Real LangChain with GPT-4

**Infrastructure:**
```
3 agent pods: 0.5 CPU Ã— 3 Ã— 730 hrs Ã— $0.04/hr = $43.80
With spot instances (70% off): $13.14
```

**LLM Costs:**
```
Assumptions:
- 1000 investigations/month
- 150 tokens average per investigation
- GPT-4: $0.002 per 1K tokens

Monthly LLM cost: 1000 Ã— 150 / 1000 Ã— $0.002 = $0.30
```

**Total: ~$13-15/month** with spot instances

### Cost Optimization Strategies

**1. Caching:**
```python
# Cache common investigation patterns
if alert_type in cache:
    return cached_result  # Save LLM call
```

**2. Model selection:**
```
GPT-4: $0.002/1K tokens - Use for complex investigations
GPT-3.5: $0.0002/1K tokens - Use for simple alerts (90% savings!)
```

**3. Prompt optimization:**
```
Verbose prompt: 500 tokens
Optimized prompt: 100 tokens
Savings: 80%
```

---

## ğŸ“ Key Learning Outcomes

### Conceptual Understanding

After completing this lab, you understand:

âœ… **LangChain Production Patterns:**
- Chain-style execution (Plannerâ†’Executorâ†’Reasoner)
- Tool orchestration in production
- State management across requests
- Cost tracking and optimization

âœ… **Operational Patterns:**
- Horizontal scaling for ML agents
- Metrics exposure for observability
- Resource management
- Health monitoring

âœ… **Cost Management:**
- Token-level tracking
- Real-time cost calculation
- Budget management
- Optimization strategies

âœ… **Infrastructure Design:**
- Multi-replica deployments
- Autoscaling based on load
- Zero-downtime updates
- Prometheus integration

### Technical Skills

You can now:

âœ… **Deploy LangChain-style agents** in production
âœ… **Implement chain execution** patterns
âœ… **Track and optimize costs** at token level
âœ… **Expose Prometheus metrics** for ML systems
âœ… **Configure horizontal scaling** for agents
âœ… **Manage agent state** in memory
âœ… **Debug chain execution** flows

### Real-World Patterns

You've learned:

âœ… **Chain orchestration** - Multi-step agent workflows
âœ… **Cost-aware AI** - Financial intelligence in agents
âœ… **Production scaling** - High-availability patterns
âœ… **Observability** - Metrics for AI systems

---

## ğŸ”§ Troubleshooting

### Issue: Metrics Not Showing

**Check metrics endpoint:**
```bash
curl http://localhost:8000/metrics
```

**Verify Prometheus format:**
```bash
curl http://localhost:8000/metrics | grep "# HELP"
```

### Issue: High Memory Usage

**Check pod memory:**
```bash
kubectl top pod -n langchain-free
```

**Adjust limits:**
```yaml
resources:
  limits:
    memory: "512Mi"  # Increase if needed
```

---

## ğŸ§¹ Cleanup

```bash
kubectl delete namespace langchain-free
kind delete cluster --name langchain-free
```

---

## ğŸ“š Next Steps

### Explore PAID Version

The PAID version adds:
- **Real LangChain** integration
- **Redis** conversation memory
- **Postgres** decision logs
- **Vector DB** semantic memory
- **OpenTelemetry** tracing
- **Grafana** dashboards

---

## ğŸ‰ Congratulations!

You've mastered LangChain production patterns!

### What You've Learned:

âœ… **Chain Execution** - Multi-step agent workflows  
âœ… **Cost Tracking** - Token-level financial intelligence  
âœ… **Production Scaling** - Horizontal pod autoscaling  
âœ… **Observability** - Prometheus metrics for AI  

You now understand how to run LangChain agents in production!

Happy learning! ğŸš€ğŸ”—ğŸ¤–ğŸ’°